# -*- coding: utf-8 -*-
"""Tugas Akhir.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-mg08rJuz3pbsMBPv04Ua58tfGjNXC3
"""

!pip install Sastrawi
!pip install torch
!pip install torch-geometric

import pandas as pd
import re
import string
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from nltk import FreqDist
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from collections import Counter
import networkx as nx
import matplotlib.pyplot as plt
import math
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import Counter
import networkx as nx
from nltk import bigrams
from nltk.probability import FreqDist
from math import log
from collections import defaultdict
import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import numpy as np

from google.colab import files
uploaded = files.upload()

data  = pd.read_csv('Datase_Shopee 1 4.csv')

jumlah_baris, jumlah_kolom = data.shape

print("jumlah baris : ",jumlah_baris)
print("jumlah kolom : ",jumlah_kolom)

# Membuat DataFrame
data = pd.DataFrame(data)

# Cek missing value dan identifikasi baris yang mengalami missing value
missing_rows = data[data.isnull().any(axis=1)].index.tolist()

# Tampilkan indeks baris yang mengalami missing value
print("Baris yang mengalami missing value:", missing_rows)

# Konversi ke huruf kecil
data['lower_textfull'] = data['Ulasan'].str.lower()
#Menghapus Emoticon dan Tanda Baca
def cleaningtext(text):
  text = re.sub(r'[0-9]+', '', text)  # Hapus angka
  text = re.sub(r'\n', ' ', text)  # Hapus baris baru
  text = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U0001F60D]', '', text)  # Hapus emotikon
  text = re.sub(r'[^\w\s]', '', text)  # Hapus tanda baca
  text = re.sub(r'Â²', '', text)  # Hapus karakter khusus seperti tanda ulang
  text = re.sub(r'_', '', text)  # Hapus tanda garis bawah
  return text

data['cleaning'] = data['lower_textfull'].apply(cleaningtext)
data

# Kamus kata tidak baku ke kata baku (contoh sederhana)
kamus_Non_Baku = {
    "brg" : "barang",
    "brgnya" : "barangnya",
    "aminn" : "amin",
    "chatnya" : "chat",
    "balasin" : "balas",

}

# Fungsi untuk normalisasi teks
def normalisasi_teks(teks):
    # Ganti kata tidak baku dengan kata baku
    teks = ' '.join([kamus_Non_Baku.get(kata, kata) for kata in teks.split()])
    return teks

# Asumsikan 'data' adalah DataFrame dengan kolom 'Ulasan_New' yang berisi data ulasan
data['normal'] = data['cleaning'].apply(normalisasi_teks)
data

# Definisikan daftar stopwords khusus
custom_stopwords = {
    "ini", "adalah", "di", "dan", "ke", "yang", "dalam", "untuk", "dengan", "atau", "dari",
    "sebagai", "bahwa", "pada", "oleh", "dapat", "karena", "juga", "dalam", "saat", "itu", "sih", "bro", "coy", "amin"
}

# Fungsi untuk menghapus stopwords dari teks
def remove_stopwords(text, stop_words):
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    return ' '.join(filtered_tokens)

# Menghapus stopwords dari setiap teks dalam kolom 'reviews'
data['StopWord'] = data['normal'].apply(lambda x: remove_stopwords(x, custom_stopwords) if x.strip() else '')

# Tampilkan hasil setelah penghapusan stopwords
data

# Buat stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Fungsi untuk stemming
def stemming_teks(teks):
    return stemmer.stem(teks)

# Terapkan stemming pada kolom 'Ulasan'
data['Ulasan_Stemmed'] = data['StopWord'].apply(stemming_teks)

# Tampilkan DataFrame setelah stemming
data

data = data.drop(['Ulasan', 'lower_textfull', 'cleaning', 'normal', 'StopWord'], axis=1)
data = data.rename(columns={'Ulasan_Stemmed': 'reviews'})
columns_order = ['Time', 'reviews', 'Fake', 'Not Fake']
data = data[columns_order]
data

# Fungsi untuk menghitung jumlah kata
def count_words(text):
    return len(text.split())

data['word_count'] = data['reviews'].apply(count_words)
data

import nltk
from nltk.probability import FreqDist
import networkx as nx
import matplotlib.pyplot as plt

# Pastikan NLTK data telah diunduh
nltk.download('punkt')

# Fungsi untuk tokenisasi dan menghitung frekuensi kata dalam satu teks
def tokenize_and_count(text):
    tokens = nltk.word_tokenize(text)
    return FreqDist(tokens)

# Membuat grafik NetworkX
G = nx.Graph()

# Tambahkan node untuk setiap kata unik dalam setiap dokumen
for i, (text, fake_label) in enumerate(zip(data['reviews'], data['Fake']), 1):
    if text.strip():  # Hanya memproses teks yang tidak kosong
        freq_dist = tokenize_and_count(text)
        for word in freq_dist.keys():
            if not G.has_node(word):  # Cek apakah node sudah ada
                G.add_node(word, color='blue')  # Tambahkan node baru

        # Tambahkan node untuk dokumen dengan isi kalimat ulasan dan label
        label = 'F' if fake_label == 'yes' else 'NF'
        G.add_node(text, color='red', label=label)

# Menghitung jumlah node berdasarkan warna
num_red_nodes = sum(1 for node in G.nodes if G.nodes[node]['color'] == 'red')
num_blue_nodes = sum(1 for node in G.nodes if G.nodes[node]['color'] == 'blue')

# Tampilkan hasil jumlah node
print(f"Jumlah node berwarna merah: {num_red_nodes}")
print(f"Jumlah node berwarna biru: {num_blue_nodes}")

# Tampilkan grafik
plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G)
node_labels = {node: G.nodes[node].get('label', '') for node in G.nodes}
nx.draw(G, pos, with_labels=True, node_color=[G.nodes[node]['color'] for node in G.nodes], labels=node_labels, font_size=12, font_weight="bold")
plt.show()

import nltk
from nltk.probability import FreqDist
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import math
from nltk.util import bigrams
from collections import defaultdict

# Pastikan NLTK data telah diunduh
nltk.download('punkt')

# Fungsi untuk tokenisasi dan menghitung frekuensi kata dalam satu teks
def tokenize_and_count(text):
    tokens = nltk.word_tokenize(text)
    return FreqDist(tokens)

# Fungsi untuk menghitung TF
def compute_tf(text):
    freq_dist = tokenize_and_count(text)
    total_terms = sum(freq_dist.values())
    tf = {word: count / total_terms for word, count in freq_dist.items()}
    return tf

# Fungsi untuk menghitung IDF
def compute_idf(data):
    df_count = {}
    total_docs = len(data['reviews'])

    for text in data['reviews']:
        if text.strip():  # Hanya memproses teks yang tidak kosong
            words = set(tokenize_and_count(text).keys())
            for word in words:
                df_count[word] = df_count.get(word, 0) + 1

    idf = {word: math.log(total_docs / df_count[word]) for word in df_count}
    return idf

# Menghitung frekuensi kata dan bigram untuk setiap teks dalam kolom 'reviews'
def calculate_frequencies(texts):
    word_freq = FreqDist()
    bigram_freq = defaultdict(int)
    total_bigrams = 0

    for text in texts:
        if text.strip():  # Hanya memproses teks yang tidak kosong
            freq_dist = tokenize_and_count(text)
            word_freq.update(freq_dist)
            bigrams_in_text = list(bigrams(freq_dist.keys()))
            for bigram in bigrams_in_text:
                bigram_freq[bigram] += 1
            total_bigrams += len(bigrams_in_text)

    #for bigram, freq in bigram_freq.items():
     #   print(f"Bigram: {bigram} - Frekuensi: {freq}")

    total_words = sum(word_freq.values())
    return word_freq, bigram_freq, total_words, total_bigrams

# Fungsi untuk menghitung PMI
def compute_pmi(bigram, word_freq, bigram_freq, total_words, total_bigrams):
    w1, w2 = bigram
    p_w1 = word_freq[w1] / total_words
    p_w2 = word_freq[w2] / total_words
    p_w1_w2 = bigram_freq[bigram] / total_bigrams
    pmi = math.log(p_w1_w2 / (p_w1 * p_w2))
    return pmi

# Membuat grafik NetworkX
G = nx.Graph()

# Tambahkan node untuk setiap kata unik dalam setiap dokumen
for i, (text, fake_label) in enumerate(zip(data['reviews'], data['Fake']), 1):
    if text.strip():  # Hanya memproses teks yang tidak kosong
        freq_dist = tokenize_and_count(text)
        for word in freq_dist.keys():
            if not G.has_node(word):  # Cek apakah node sudah ada
                G.add_node(word, color='blue')  # Tambahkan node baru

        # Tambahkan node untuk dokumen dengan isi kalimat ulasan dan label
        label = 'F' if fake_label == 'yes' else 'NF'
        G.add_node(text, color='red', label=label)

# Menghitung jumlah node berdasarkan warna
num_red_nodes = sum(1 for node in G.nodes if G.nodes[node]['color'] == 'red')
num_blue_nodes = sum(1 for node in G.nodes if G.nodes[node]['color'] == 'blue')

# Tampilkan hasil jumlah node
print(f"Jumlah node berwarna merah: {num_red_nodes}")
print(f"Jumlah node berwarna biru: {num_blue_nodes}")

# Menghitung IDF
idf = compute_idf(data)

# Menambahkan edges berdasarkan TF-IDF
for text in data['reviews']:
    if text.strip():  # Hanya memproses teks yang tidak kosong
        tf = compute_tf(text)
        for word, tf_value in tf.items():
            if G.has_node(word) and G.has_node(text):
                tf_idf = tf_value * idf[word]
                G.add_edge(text, word, weight=tf_idf)

# Menghitung frekuensi kata dan bigram
word_freq, bigram_freq, total_words, total_bigrams = calculate_frequencies(data['reviews'])

# Menambahkan edges berdasarkan PMI
for bigram in bigram_freq:
    pmi = compute_pmi(bigram, word_freq, bigram_freq, total_words, total_bigrams)
    w1, w2 = bigram
    if G.has_node(w1) and G.has_node(w2):
        G.add_edge(w1, w2, weight=pmi)

# Tampilkan grafik dengan edges
plt.figure(figsize=(15, 15))  # Menyesuaikan ukuran figure
pos = nx.spring_layout(G, k=0.8)  # Mengatur parameter k untuk memperbesar jarak antar node
node_labels = {node: G.nodes[node].get('label', '') for node in G.nodes}
edges = G.edges(data=True)
weights = [edge[2]['weight'] for edge in edges]
nx.draw(G, pos, with_labels=True, node_color=[G.nodes[node]['color'] for node in G.nodes], labels=node_labels, font_size=10, font_weight="bold", width=weights)
plt.show()

import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, f1_score, precision_score, recall_score, confusion_matrix

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)  # Menggunakan seed 42, Anda dapat memilih seed lain jika diinginkan

# Fungsi untuk menambahkan loop diri pada matriks adjacency
def add_self_loops(adj):
    eye = np.eye(adj.shape[0])
    adj_with_loops = adj + eye
    print(f"Ukuran matriks adjacency setelah penambahan loop diri: {adj_with_loops.shape}")
    return adj_with_loops

# Fungsi untuk membuat matriks fitur
def create_feature_matrix(G):
    feature_matrix = np.array([[1] for _ in G.nodes])
    print("Ukuran matriks fitur:", feature_matrix.shape)
    return feature_matrix

# Fungsi untuk mengonversi graf NetworkX ke format PyTorch Geometric
def create_torch_data(G):
    A = nx.adjacency_matrix(G).todense()
    A = add_self_loops(A)
    X = create_feature_matrix(G)

    edge_index = torch.tensor(np.array(np.nonzero(A)), dtype=torch.long)
    edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)
    edge_attr = torch.tensor(A[np.nonzero(A)], dtype=torch.float)

    x = torch.tensor(X, dtype=torch.float)

    # Hanya node merah yang memiliki label
    labels = []
    mask = []
    for node in G.nodes:
        if 'label' in G.nodes[node]:
            labels.append(G.nodes[node]['label'])
            mask.append(True)
        else:
            mask.append(False)

    # Konversi labels menjadi tensor
    label_map = {'F': 1, 'NF': 0}
    labels = torch.tensor([label_map[label] for label in labels], dtype=torch.long)

    # Konversi mask menjadi tensor boolean
    mask = torch.tensor(mask, dtype=torch.bool)

    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=labels, mask=mask)
    return data

def split_data(data, train_ratio=0.80):
    num_red_nodes = data.y.size(0)  # Jumlah node merah yang memiliki label
    indices = torch.arange(num_red_nodes)
    indices = torch.randperm(num_red_nodes)

    train_size = int(num_red_nodes * train_ratio)
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]

    train_mask = torch.zeros_like(data.mask, dtype=torch.bool)
    test_mask = torch.zeros_like(data.mask, dtype=torch.bool)

    # Pastikan hanya node merah yang diambil dengan data.mask.nonzero(as_tuple=True)[0]
    train_mask[data.mask.nonzero(as_tuple=True)[0][train_indices]] = 1
    test_mask[data.mask.nonzero(as_tuple=True)[0][test_indices]] = 1

    data.train_mask = train_mask
    data.test_mask = test_mask

    # Cek ukuran mask dan tensor
    print(f"Ukuran mask latih: {train_mask.size()}")
    print(f"Ukuran mask uji: {test_mask.size()}")
    print(f"Ukuran tensor fitur: {data.x.size()}")
    print(f"Ukuran tensor label: {data.y.size()}")

    # Cek jumlah data latih dan uji
    num_train = train_mask.sum().item()
    num_test = test_mask.sum().item()
    print(f"Jumlah data latih: {num_train}")
    print(f"Jumlah data uji: {num_test}")

    # Mengecek label di data uji
    # Hanya pilih test_labels dari node merah yang memiliki label
    test_labels = data.y[test_indices]
    num_fake_in_test = (test_labels == 1).sum().item()
    num_not_fake_in_test = (test_labels == 0).sum().item()

    print(f"Jumlah label 'F' (Fake) di data uji: {num_fake_in_test}")
    print(f"Jumlah label 'NF' (Not Fake) di data uji: {num_not_fake_in_test}")



# Model GCN
class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x


# Mengonversi NetworkX graph ke PyTorch Geometric data
data_pg = create_torch_data(G)
split_data(data_pg)  # Membagi data menjadi data uji dan latih

# Parameter model
in_channels = data_pg.x.shape[1]
hidden_channels = 16
out_channels = 2  # Output dimensi, disesuaikan jika diperlukan

# Initialize model, optimizer
model = GCN(in_channels, hidden_channels, out_channels)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training function
def train(data):
    model.train()
    optimizer.zero_grad()
    out = model(data)

    # Hanya gunakan data dari node merah yang masuk ke dalam train_mask
    out = out[data.train_mask]
    labels = data.y[:out.size(0)]

    loss = F.cross_entropy(out, labels)
    loss.backward()
    optimizer.step()
    return loss.item()

# Save loss values
losses = []

for epoch in range(100):
    loss = train(data_pg)
    losses.append(loss)

# Plot Loss Curve
plt.figure(figsize=(10, 5))
plt.plot(range(1,101), losses, marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curve During Training')
plt.grid(True)
plt.show()

# Lakukan forward pass untuk mendapatkan prediksi node
model.eval()
with torch.no_grad():
    logits = model(data_pg)  # Ukuran logits akan menjadi [num_nodes, out_channels]

    # Print ukuran logits
    print(f"Ukuran logits: {logits.shape}")

    # Prediksi diambil dari argmax logits untuk setiap node
    predictions = torch.argmax(logits, dim=1).numpy()

    # Print ukuran predictions
    print(f"Ukuran predictions: {predictions.shape}")

    # Tampilkan nilai logit dari setiap node
    for i, node in enumerate(G.nodes()):
        logit_values = logits[i].numpy()
        predicted_label = 'F' if predictions[i] == 1 else 'NF'
        print(f"Node {node}: Logit = {logit_values}, Prediksi = {predicted_label}")

    # Konversi predictions dan labels ke tensor PyTorch
    predictions_tensor = torch.tensor(predictions, dtype=torch.long)
    labels_tensor = data_pg.y

    # Mask untuk hanya memeriksa prediksi untuk node merah pada data uji
    pred_red_nodes = predictions_tensor[data_pg.test_mask]
    labels_red_nodes = labels_tensor[:pred_red_nodes.size(0)]

    # Hitung matriks kebingungan
    conf_matrix = confusion_matrix(labels_red_nodes.numpy(), pred_red_nodes.numpy())
    print(f"Matriks Kebingungan:\n{conf_matrix}")

    # Hitung precision, recall, dan F1 score
    precision = precision_score(labels_red_nodes.numpy(), pred_red_nodes.numpy(), average='weighted')
    recall = recall_score(labels_red_nodes.numpy(), pred_red_nodes.numpy(), average='weighted')
    f1 = f1_score(labels_red_nodes.numpy(), pred_red_nodes.numpy(), average='weighted')

    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Nilai F1 Score: {f1:.4f}")



# Menampilkan prediksi node
for i, node in enumerate(G.nodes()):
    if 'label' in G.nodes[node]:
        original_label = G.nodes[node]['label']
    else:
        original_label = "Node tidak memiliki label"

    predicted_label = 'F' if predictions[i] == 1 else 'NF'
    print(f"Node {node}: Label asli = {original_label}, Prediksi = {predicted_label}")